{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "drive_mount = ''\n",
        "data_dir=''"
      ],
      "metadata": {
        "id": "UPAOP9DT2faZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(drive_mount)"
      ],
      "metadata": {
        "id": "rvEjDwrpCpJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(data_dir)"
      ],
      "metadata": {
        "id": "CcxBn5XbEm-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pwd"
      ],
      "metadata": {
        "id": "ghciUrstEV6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "pd.set_option('display.max_rows', 500)"
      ],
      "metadata": {
        "id": "8VsPiqzxFbfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"train.csv\")"
      ],
      "metadata": {
        "id": "l9i8C9ASEfUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df.head()"
      ],
      "metadata": {
        "id": "QxiSC0oAFdl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape"
      ],
      "metadata": {
        "id": "zebZqzD4HjXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.iloc[0]['prompt']"
      ],
      "metadata": {
        "id": "wVToD1tpFpYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval(train_df.iloc[0]['prompt'])"
      ],
      "metadata": {
        "id": "uXDE2jaHJb7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.iloc[0]['response_a']"
      ],
      "metadata": {
        "id": "E3T9-55FGJSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.iloc[0]['response_b']"
      ],
      "metadata": {
        "id": "0fEXUpD-FfUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Categorize the winner model result\n",
        "train_df['winner_result'] = train_df[['winner_model_a','winner_model_b','winner_tie']].idxmax(axis=1)\n",
        "train_df['winner_model_name'] = np.where(train_df['winner_result']=='winner_model_a',train_df['model_a'],\n",
        "                                         np.where(train_df['winner_result']=='winner_model_b',train_df['model_b'],'winner_tie'))\n",
        "train_df['label'] = np.where(train_df['winner_result'] == 'winner_model_a', 0,\n",
        "                             np.where(train_df['winner_result']=='winner_model_b',1,2))"
      ],
      "metadata": {
        "id": "jAMsLn8RGMf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "_5OQfYIAa8kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['prompt'] = train_df['prompt'].apply(lambda x: eval(x))\n",
        "train_df['response_a'] = train_df['response_a'].apply(lambda x: eval(x.replace(\"null\",\"np.nan\")))\n",
        "train_df['response_b'] = train_df['response_b'].apply(lambda x:eval(x.replace(\"null\",\"np.nan\")))"
      ],
      "metadata": {
        "id": "BJ3b_uuJS5gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.explode(['prompt', 'response_a','response_b'])"
      ],
      "metadata": {
        "id": "cms_tQtTS-ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "CENnANtIIH1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EAD\n",
        "* Encode failure: Some model fail in the middle of a prompt session, but still become the winner model\n"
      ],
      "metadata": {
        "id": "2MQk_PvUZ48m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['RowNum'] = train_df.groupby('id').cumcount() + 1\n",
        "train_df['session'] = train_df['id'].astype(str) + '_' + train_df['RowNum'].astype(str)\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "L1baoth8VgrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.loc[train_df['response_b'].isnull()].head()"
      ],
      "metadata": {
        "id": "YNUz4PVbUKtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.loc[train_df['id'] == 57180984]"
      ],
      "metadata": {
        "id": "WpmaScp2L9k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.loc[train_df['id'] == 134445396]"
      ],
      "metadata": {
        "id": "PsUgwcftWWNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['encode_fail_a'] = np.where(train_df['response_a'].isnull(),1,0)\n",
        "train_df['encode_fail_b'] = np.where(train_df['response_b'].isnull(),1,0)"
      ],
      "metadata": {
        "id": "OTl4rewDWZBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df.groupby('RowNum')[['encode_fail_a','encode_fail_b']].value_counts() ## Does not seems like longer session result in more failure"
      ],
      "metadata": {
        "id": "P5UwIJRsX-QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['max_session'] = train_df.groupby('id')['RowNum'].transform('max')"
      ],
      "metadata": {
        "id": "a8pHSkl0YCP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Some model fail in the middle session but still won\n",
        "# train_df.query(\"encode_fail_a == 1 or encode_fail_b == 1 and max_session>RowNum\") #.groupby('winner_model_name').count()"
      ],
      "metadata": {
        "id": "zDRlxccvYgd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Which model is more likely to fail?\n",
        "train_df['fail_model'] = np.where((train_df['encode_fail_a']==1) & (train_df['encode_fail_b']==1),'both_fail',\n",
        "                                  np.where((train_df['encode_fail_a']==1) & (train_df['encode_fail_b']==0), train_df['model_a'],\n",
        "                                  np.where((train_df['encode_fail_a']==0) & (train_df['encode_fail_b']==1), train_df['model_b'], 'none_fail' )))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LF7ztObhZAFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['fail_model'].value_counts(normalize=True).iloc[1:].plot(kind='bar')"
      ],
      "metadata": {
        "id": "6BiLRj8AdMp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Which two models are more likely to be compared together?"
      ],
      "metadata": {
        "id": "bMrMniYZdlon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets remove the data where both model fail and the result is not tie\n",
        "train_df = train_df.drop(train_df.query(\"fail_model == 'both_fail' and label!=2\").index,axis=0)\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "JsF4PSZWeYyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df.loc[train_df['response_a'].isnull()]"
      ],
      "metadata": {
        "id": "1B_G_mjhgJV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.loc[train_df['id']==16350735][['response_a']]"
      ],
      "metadata": {
        "id": "eZMtOhp8nbiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We can treat the problem like a multiple choice classification problem"
      ],
      "metadata": {
        "id": "1uLPdR-PklUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_options(row):\n",
        "  if row['response_a']=='NaN':\n",
        "    res_a = ''\n",
        "  else:\n",
        "    res_a = row['response_a']\n",
        "\n",
        "  if row['response_b']=='NaN':\n",
        "    res_b = ''\n",
        "  else:\n",
        "    res_b = row['response_b']\n",
        "  return [res_a,res_b,'both']\n",
        "\n",
        "train_df['options'] = train_df.apply(lambda x:get_options(x),axis=1)"
      ],
      "metadata": {
        "id": "UfOt4R-hkohD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "GbnnJdXyn29U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a model for the multiple choice classification"
      ],
      "metadata": {
        "id": "s8l-7e4frk2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForMultipleChoice, get_scheduler\n",
        "# from transformers.optimization import AdamW\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "V7xlP6S5n7T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForMultipleChoice.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "yts0wX1vpRux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples = train_df[['session','prompt','options','label']].to_dict(orient='records')"
      ],
      "metadata": {
        "id": "QekMnUG_rpB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Dataset class\n",
        "class MultipleChoiceDataset(Dataset):\n",
        "    def __init__(self, examples, tokenizer, max_length=128):\n",
        "        self.examples = examples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.examples[idx]\n",
        "        prompt = example[\"prompt\"]\n",
        "        options = example[\"options\"]\n",
        "        label = example[\"label\"]\n",
        "\n",
        "        # Encode each (context + choice) spair\n",
        "        inputs = self.tokenizer(\n",
        "            [str(prompt) + \" \" + str(option) for option in options],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Shape: (num_choices, seq_len)\n",
        "        input_ids = inputs[\"input_ids\"]\n",
        "        attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": torch.tensor(label)\n",
        "        }\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OJfxE4nMn-HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MultipleChoiceDataset(train_examples, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=20)"
      ],
      "metadata": {
        "id": "BQecLxDYqRX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.examples[0]"
      ],
      "metadata": {
        "id": "vylg4rA1x-NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset and dataloader\n",
        "\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_training_steps = len(dataloader) * 3  # epochs = 3\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    loop = tqdm(dataloader, leave=True)\n",
        "    for batch in loop:\n",
        "        input_ids = batch[\"input_ids\"].to(device)       # shape: (batch_size, num_choices, seq_len)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loop.set_description(f\"Epoch {epoch}\")\n",
        "        loop.set_postfix(loss=loss.item())\n"
      ],
      "metadata": {
        "id": "xcuk16JZo3e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, './entire_model.pth')"
      ],
      "metadata": {
        "id": "ZvGFyxYDTzBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset.examples['prompt']"
      ],
      "metadata": {
        "id": "VT-9lwSUqSAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "IApJo1nbqLR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_with_probs(prompt, options):\n",
        "    # Prepare input\n",
        "    inputs = tokenizer(\n",
        "        [str(prompt) + \" \" + str(option) for option in options],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Reshape to (1, num_choices, seq_len)\n",
        "    input_ids = inputs[\"input_ids\"].unsqueeze(0).to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits  # shape: (1, num_choices)\n",
        "        probs = F.softmax(logits, dim=1)  # softmax over choices\n",
        "        predicted_index = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "    prob_values = probs[0].cpu().tolist()\n",
        "\n",
        "    # Return choice index, answer text, and probabilities\n",
        "    return {\n",
        "        \"predicted_index\": predicted_index,\n",
        "        \"predicted_choice\": options[predicted_index],\n",
        "        \"probabilities\": list(zip(options, prob_values))\n",
        "    }\n"
      ],
      "metadata": {
        "id": "8QYlIq5sq9wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = train_df.iloc[0]['prompt']\n",
        "options= train_df.iloc[0]['options']\n",
        "\n",
        "result = predict_with_probs( prompt, options)\n",
        "\n",
        "print(f\"Predicted choice: {result['predicted_choice']} (index {result['predicted_index']})\")\n",
        "print(\"Probabilities:\")\n",
        "for i, (choice, prob) in enumerate(result[\"probabilities\"]):\n",
        "    print(f\"option {i}: {prob:.4f} \\n {choice}\")"
      ],
      "metadata": {
        "id": "07kVj0LdrSKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result['probabilities'][2][1]"
      ],
      "metadata": {
        "id": "hoDtSyHyvNe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_result(row):\n",
        "  result = predict_with_probs(row['prompt'],row['options'])\n",
        "\n",
        "\n",
        "  return {'predicted_label': result['predicted_index'], 'predicted_prefered_response': result['predicted_choice'], 'predicted_prob_a':result['probabilities'][0][1], 'predicted_prob_b':result['probabilities'][1][1], 'predicted_prob_both':result['probabilities'][2][1] }"
      ],
      "metadata": {
        "id": "fcyx9ZUYsnHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[['pred_label','pred_response','pred_a','pred_b','pred_both']] = train_df.apply(lambda x:get_result(x), axis=1, result_type='expand')\n"
      ],
      "metadata": {
        "id": "-KhXnkEkrdim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, ConfusionMatrixDisplay, classification_report\n",
        "def get_performance(pred, truth):\n",
        "  cm = confusion_matrix(truth, pred)\n",
        "  accuracy = accuracy_score(truth, pred)\n",
        "  precision = precision_score(truth, pred, average='weighted')\n",
        "  recall = recall_score(truth, pred, average='weighted')\n",
        "  f1 = f1_score(truth, pred, average='weighted')\n",
        "  target_names = ['response_a', 'response_b', 'tied']\n",
        "  print(classification_report(truth, pred, target_names=target_names))\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
        "  disp.plot()\n",
        "  plt.show()\n",
        "  return cm, accuracy, precision, recall, f1\n"
      ],
      "metadata": {
        "id": "Eh-XR_JIsRsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm, accuracy, precision, recall, f1 = get_performance(train_df['pred_label'],train_df['label'])"
      ],
      "metadata": {
        "id": "xtj7yTzpyOXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy"
      ],
      "metadata": {
        "id": "uwWnXewnyP34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q9_YXRznTzZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JJ5AGj3W-yz6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}